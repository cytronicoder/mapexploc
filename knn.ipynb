{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2689f0e9",
   "metadata": {},
   "source": [
    "# Baseline k-Nearest Neighbors Classifier\n",
    "\n",
    "We first create a baseline k-Nearest Neighbors classifier, which will serve as a reference for our classification task. We define our hyperparameter grid to search over `n_neighbors` values of 3, 5, and 7, and weights options 'uniform' and 'distance'. We use `GridSearchCV` with 5-fold cross-validation and the 'f1_weighted' scoring metric to tune our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe299ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"results/models\"\n",
    "OUT_VIS = \"results/figures\"\n",
    "OUT_CSV = \"results/csv\"\n",
    "for d in (OUT_DIR, OUT_VIS, OUT_CSV):\n",
    "    os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6178fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X_train = pd.read_csv(\"data/processed/X/train.csv\")\n",
    "    X_val = pd.read_csv(\"data/processed/X/val.csv\")\n",
    "    logging.info(f\"Loaded features: X_train {X_train.shape}, X_val {X_val.shape}\")\n",
    "\n",
    "    y_train_df = pd.read_csv(\"data/processed/Y/train.csv\")\n",
    "    y_val_df = pd.read_csv(\"data/processed/Y/val.csv\")\n",
    "    logging.info(f\"Loaded labels: y_train {y_train_df.shape}, y_val {y_val_df.shape}\")\n",
    "\n",
    "    y_train = y_train_df.iloc[:, 0]\n",
    "    y_val = y_val_df.iloc[:, 0]\n",
    "    logging.info(f\"Initial y_train classes: {sorted(y_train.unique())}\")\n",
    "\n",
    "    X_train = X_train.select_dtypes(include=[np.number])\n",
    "    X_val = X_val.select_dtypes(include=[np.number])\n",
    "    logging.info(f\"Numeric filter: X_train {X_train.shape}, X_val {X_val.shape}\")\n",
    "\n",
    "    threshold = 50\n",
    "    counts = y_train.value_counts()\n",
    "    logging.info(f\"Pre-merge class counts: {counts.to_dict()}\")\n",
    "    rare = counts[counts < threshold].index.tolist()\n",
    "    if rare:\n",
    "        y_train = y_train.replace({cls: \"Other\" for cls in rare})\n",
    "        y_val = y_val.replace({cls: \"Other\" for cls in rare})\n",
    "        logging.info(f\"Merged rare classes: {rare} -> 'Other'\")\n",
    "    else:\n",
    "        logging.info(\"No rare classes to merge.\")\n",
    "    logging.info(f\"Post-merge classes: {sorted(y_train.unique())}\")\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(X_train, y_train):\n",
    "    pipeline = Pipeline([(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier())])\n",
    "    param_grid = {\n",
    "        \"knn__n_neighbors\": [3, 5, 7],\n",
    "        \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    logging.info(\"Starting k-NN GridSearchCV...\")\n",
    "    grid.fit(X_train, y_train)\n",
    "    logging.info(f\"Best params: {grid.best_params_}\")\n",
    "    logging.info(f\"Best CV F1-weighted: {grid.best_score_:.4f}\")\n",
    "    return grid.best_estimator_, grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9236110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, X, y, out_csv=OUT_CSV, out_vis=OUT_VIS):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    f1w = f1_score(y, y_pred, average=\"weighted\")\n",
    "    logging.info(f\"Validation accuracy: {acc:.4f}\")\n",
    "    logging.info(f\"Validation F1-weighted: {f1w:.4f}\")\n",
    "\n",
    "    # Save classification report\n",
    "    report = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(f\"{out_csv}/knn_classification_report.csv\")\n",
    "\n",
    "    # Save confusion matrix\n",
    "    classes = model.classes_\n",
    "    cm = confusion_matrix(y, y_pred, labels=classes)\n",
    "    cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "    cm_df.to_csv(f\"{out_csv}/knn_confusion_matrix.csv\")\n",
    "\n",
    "    # Save ROC data\n",
    "    y_bin = label_binarize(y, classes=classes)\n",
    "    pred_bin = label_binarize(y_pred, classes=classes)\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], pred_bin[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        pd.DataFrame({\"fpr\": fpr[i], \"tpr\": tpr[i]}).to_csv(\n",
    "            f\"{out_csv}/knn_roc_curve_{cls}.csv\", index=False\n",
    "        )\n",
    "\n",
    "    # Plot overall ROC\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, cls in enumerate(classes):\n",
    "        plt.plot(fpr[i], tpr[i], label=f\"{cls} (AUC={roc_auc[i]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"k-NN ROC Curves (One-vs-Rest)\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_vis}/knn_roc_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    return acc, f1w, classes, roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a794e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_k_values(X_train, y_train, X_val, y_val, ks=[3, 5, 7]):\n",
    "    records = []\n",
    "    for k in ks:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=k))]\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        f1w = f1_score(y_val, y_pred, average=\"weighted\")\n",
    "        records.append({\"k\": k, \"accuracy\": acc, \"f1_weighted\": f1w})\n",
    "    df = pd.DataFrame(records)\n",
    "    # Plot\n",
    "    df.set_index(\"k\").plot.bar(rot=0)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"k-NN Benchmark: Accuracy & Weighted-F1 by k\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUT_VIS}/knn_benchmark_ks.png\", dpi=150)\n",
    "    plt.close()\n",
    "    df.to_csv(f\"{OUT_CSV}/knn_benchmark_ks.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c7c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_subset(model, X, y, classes, out_vis=OUT_VIS):\n",
    "    y_pred = model.predict(X)\n",
    "    rpt = classification_report(y, y_pred, output_dict=True, zero_division=0)\n",
    "    df_rpt = pd.DataFrame(rpt).transpose()\n",
    "    # select two lowest-recall classes with at least 5 samples\n",
    "    df_rpt[\"recall\"] = df_rpt[\"recall\"]\n",
    "    eligible = df_rpt.loc[(df_rpt.index.isin(classes)) & (df_rpt[\"support\"] >= 5)]\n",
    "    worst = eligible.sort_values(\"recall\").head(2).index.tolist()\n",
    "    for cls in worst:\n",
    "        # create binary confusion: cls vs rest\n",
    "        binary_y = (y == cls).astype(int)\n",
    "        binary_pred = (y_pred == cls).astype(int)\n",
    "        cm2 = confusion_matrix(binary_y, binary_pred)\n",
    "        sns.heatmap(\n",
    "            cm2,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=[f\"!{cls}\", cls],\n",
    "            yticklabels=[f\"!{cls}\", cls],\n",
    "        )\n",
    "        plt.title(f\"Binary Confusion: {cls} vs Rest\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.xlabel(\"Pred\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{out_vis}/knn_confusion_binary_{cls}.png\", dpi=150)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_bottom_roc(roc_auc, fpr, tpr, classes, out_vis=OUT_VIS):\n",
    "    # identify top and bottom\n",
    "    sorted_auc = sorted(roc_auc.items(), key=lambda x: x[1])\n",
    "    bottom = sorted_auc[0][0]\n",
    "    top = sorted_auc[-1][0]\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    for idx in [bottom, top]:\n",
    "        cls = classes[idx]\n",
    "        plt.plot(fpr[idx], tpr[idx], label=f\"{cls} (AUC={roc_auc[idx]:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(\"ROC: Worst vs Best Classes\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{out_vis}/knn_roc_worst_best.png\", dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top5_neighbors(model, X_train, y_train, X_val, sample_idx=None, n_neighbors=5):\n",
    "    if sample_idx is None:\n",
    "        sample_idx = X_val.index[0]\n",
    "    sample_vec = X_val.loc[[sample_idx]]\n",
    "    knn = model.named_steps[\"knn\"]\n",
    "    scaler = model.named_steps[\"scaler\"]\n",
    "    sample_scaled = scaler.transform(sample_vec)\n",
    "    distances, neighbors = knn.kneighbors(sample_scaled, n_neighbors=n_neighbors)\n",
    "    logging.info(f\"Sample index: {sample_idx}\")\n",
    "    for dist, nbr in zip(distances[0], neighbors[0]):\n",
    "        seq_id = X_train.index[nbr]\n",
    "        label = y_train.iloc[nbr]\n",
    "        logging.info(f\"Neighbor: {seq_id}, Label: {label}, Distance: {dist:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn, cv_results = train_knn(X_train, y_train)\n",
    "joblib.dump(best_knn, os.path.join(OUT_DIR, \"knn_baseline_best.pkl\"))\n",
    "pd.DataFrame(cv_results).to_csv(f\"{OUT_CSV}/knn_cv_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82b2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1w, classes, roc_auc, fpr, tpr = evaluate_and_save(best_knn, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb185276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bench = benchmark_k_values(X_train, y_train, X_val, y_val)\n",
    "plot_confusion_subset(best_knn, X_val, y_val, classes)\n",
    "plot_top_bottom_roc(roc_auc, fpr, tpr, classes)\n",
    "show_top5_neighbors(best_knn, X_train, y_train, X_val)\n",
    "\n",
    "logging.info(\"All analyses complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "X_test = pd.read_csv(\"data/processed/X/test.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/Y/test.csv\").squeeze()\n",
    "X_test = X_test.select_dtypes(include=[np.number])\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Measure training time\n",
    "start_train = time.time()\n",
    "best_knn.fit(X_train, y_train)\n",
    "train_time = time.time() - start_train\n",
    "print(f\"Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "# Predict and measure throughput\n",
    "start_pred = time.time()\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "pred_time = time.time() - start_pred\n",
    "throughput = len(X_test) / pred_time\n",
    "print(f\"Prediction throughput: {throughput:.0f} samples/second\")\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "f1w = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Weighted F1-Score: {f1w:.3f}\")\n",
    "\n",
    "# Per-class F1 for classes >=5% support\n",
    "report = classification_report(y_test, y_test_pred, output_dict=True, zero_division=0)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "threshold_count = len(y_test) * 0.05\n",
    "accuracy_mask = report_df.index != 'accuracy'\n",
    "support_mask = report_df['support'] >= threshold_count\n",
    "selected = report_df.loc[accuracy_mask & support_mask]\n",
    "min_f1 = selected['f1-score'].min() if not selected.empty else None\n",
    "if min_f1 is not None:\n",
    "    print(f\"Minimum per-class F1 for classes ≥5%: {min_f1:.3f}\")\n",
    "else:\n",
    "    print(\"No classes ≥5% support\")\n",
    "\n",
    "# Save detailed test report\n",
    "report_df.to_csv(f\"{OUT_CSV}/knn_test_classification_report.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eslp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
