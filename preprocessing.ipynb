{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06ac93c9",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This is a neat script that downloads UniProt data and extracts subcellular localization annotations for each protein. We'll use this data to later extract features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43b080",
   "metadata": {},
   "source": [
    "## Shell script\n",
    "\n",
    "This script downloads the UniProt data and decompresses it. Takes around 3 minutes to run. We later extract the sequences and re-write them as FASTA ourselves; however, you are more than welcome to use the original FASTA files if you prefer using:\n",
    "\n",
    "```bash\n",
    "wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n",
    "gunzip uniprot_sprot.fasta.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af20833",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "set -euo pipefail\n",
    "mkdir -p data/raw\n",
    "cd data/raw\n",
    "\n",
    "echo \"Downloading UniProtKB/Swiss-Prot...\"\n",
    "wget -q ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cd612e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "echo \"Decompressing...\"\n",
    "gunzip -f data/raw/uniprot_sprot.dat.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677033",
   "metadata": {},
   "source": [
    "## Python module\n",
    "\n",
    "The following Python module contains the code that processes the UniProt data and extracts subcellular localization annotations for each protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b134a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import SwissProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b499730",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DAT = \"data/raw/uniprot_sprot.dat\"\n",
    "INPUT_FASTA = \"data/processed/nonredundant.fasta\"\n",
    "\n",
    "OUTPUT_ANN = \"data/processed/annotations.csv\"\n",
    "OUTPUT_FASTA = \"data/processed/filtered.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df244990",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [\n",
    "    OUTPUT_ANN,\n",
    "    OUTPUT_FASTA,\n",
    "]:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25719cf9",
   "metadata": {},
   "source": [
    "We manually exclude a few terms that indicates non-experimental evidence or are not specific enough to be useful for localization prediction. Additionally, we map some specific biological locations to more general terms to reduce the number of unique labels. This should be extended as needed, as this list is far from exhaustive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954b2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_EXCLUDE_TERMS = {\"probable\", \"potential\", \"by similarity\", \"prediction\"}\n",
    "ALLOWED_LOCS = {\n",
    "    \"Cytoplasm\",\n",
    "    \"Nucleus\",\n",
    "    \"Secreted\",\n",
    "    \"Mitochondrion\",\n",
    "    \"Periplasm\",\n",
    "    \"Virion\",\n",
    "    \"Plastid\",\n",
    "    \"Membrane\",\n",
    "    \"Peroxisome\",\n",
    "    \"Endoplasmic Reticulum\",\n",
    "    \"Golgi Apparatus\",\n",
    "    \"Lysosome\",\n",
    "    \"Vacuole\",\n",
    "    \"Cell Projection\",\n",
    "    \"Cell Surface\",\n",
    "    \"Cell Junction\",\n",
    "    \"Endosome\",\n",
    "}\n",
    "_SYNONYM_MAP = {\n",
    "    \"host membrane\": \"Membrane\",\n",
    "    \"host cell membrane\": \"Membrane\",\n",
    "    \"cell membrane\": \"Membrane\",\n",
    "    \"cell outer membrane\": \"Membrane\",\n",
    "    \"plasma membrane\": \"Membrane\",\n",
    "    \"mitochondrial matrix\": \"Mitochondrion\",\n",
    "    \"mitochondrion matrix\": \"Mitochondrion\",\n",
    "    \"endoplasmic reticulum lumen\": \"Endoplasmic Reticulum\",\n",
    "    \"er lumen\": \"Endoplasmic Reticulum\",\n",
    "    \"golgi\": \"Golgi Apparatus\",\n",
    "    # extend as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24005557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_and_primary(text: str) -> str:\n",
    "    text = re.split(r\"Note=\", text, maxsplit=1)[0]\n",
    "    text = re.sub(r\"\\{.*?\\}|\\(.*?\\)\", \"\", text)\n",
    "    part = re.split(r\"[.;]\", text, maxsplit=1)[0].strip()\n",
    "    if not part:\n",
    "        return \"\"\n",
    "    low = part.lower()\n",
    "    if low in _SYNONYM_MAP:\n",
    "        canon = _SYNONYM_MAP[low]\n",
    "    else:\n",
    "        canon = part.title()\n",
    "    return canon if canon in ALLOWED_LOCS else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fed63",
   "metadata": {},
   "source": [
    "We exclude multi-compartment entries for this iteration. Single-label classifiers canâ€™t handle proteins annotated to two or more compartments - splitting multi-labels naively can inflate class counts and introduce bias.\n",
    "\n",
    "Some more considerations:\n",
    "\n",
    "+ Performance benchmarks (speed, memory) become harder to interpret when outputs are vectors rather than one label.\n",
    "+ Decision-support tools often struggle to map multi-compartment calls to single ACMG evidence codes.\n",
    "+ Rare two-compartment combinations will have very few examples. This can undermine learning.\n",
    "\n",
    "Future iterations can revisit multi-label approaches once the single-label pipeline produces a good benchmark, as clinically, mis- or multi-localization can be disease-relevant, and it is always good to retain them and preserve for downstream pathway analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protein_data(dat_file: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Parse a UniProt .dat file and return only entries with exactly one\n",
    "    experimentally-verified subcellular location from ALLOWED_LOCS.\n",
    "    \"\"\"\n",
    "    results: List[Dict[str, Any]] = []\n",
    "\n",
    "    try:\n",
    "        handle = open(dat_file)\n",
    "    except OSError as e:\n",
    "        raise RuntimeError(f\"Cannot open file {dat_file}: {e}\")\n",
    "\n",
    "    with handle:\n",
    "        for rec in SwissProt.parse(handle):\n",
    "            locs: List[str] = []\n",
    "\n",
    "            # 1) try structured API\n",
    "            if hasattr(rec, \"subcellular_locations\") and rec.subcellular_locations:\n",
    "                for loc_tuple in rec.subcellular_locations:\n",
    "                    loc = loc_tuple.location or \"\"\n",
    "                    cleaned = _clean_and_primary(loc)\n",
    "                    if cleaned:\n",
    "                        locs.append(cleaned)\n",
    "            else:\n",
    "                # 2) fallback to scanning comments\n",
    "                for comment in rec.comments:\n",
    "                    if not comment.upper().startswith(\"SUBCELLULAR LOCATION:\"):\n",
    "                        continue\n",
    "                    body = comment.split(\":\", 1)[1]\n",
    "                    for piece in re.split(r\"[;]\", body):\n",
    "                        cleaned = _clean_and_primary(piece)\n",
    "                        if cleaned:\n",
    "                            locs.append(cleaned)\n",
    "\n",
    "            # exclude non-experimental evidence\n",
    "            combined = \" \".join(rec.comments).lower()\n",
    "            if any(term in combined for term in _EXCLUDE_TERMS):\n",
    "                continue\n",
    "\n",
    "            # dedupe and require exactly one compartment\n",
    "            unique = list(dict.fromkeys(locs))\n",
    "            if len(unique) != 1:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"entry_name\": rec.entry_name,\n",
    "                    \"sequence\": rec.sequence,\n",
    "                    \"localization\": unique[0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(OUTPUT_ANN):\n",
    "    df = pd.read_csv(OUTPUT_ANN)\n",
    "    print(f\"Loaded existing annotations from {OUTPUT_ANN}\")\n",
    "else:\n",
    "    print(f\"No existing annotations found, extracting from {INPUT_DAT}\")\n",
    "    print(\"Extracting protein data...\")\n",
    "    protein_data = extract_protein_data(INPUT_DAT)\n",
    "\n",
    "    df = pd.DataFrame(protein_data)\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "\n",
    "    df.to_csv(OUTPUT_ANN, index=False)\n",
    "    print(f\"\\nWrote annotations to {OUTPUT_ANN}!\")\n",
    "    print(f\"DataFrame saved with {len(df)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b680663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(f\"Unique localizations: {df['localization'].nunique()}\")\n",
    "print(f\"Average sequence length: {df['sequence'].str.len().mean():.1f}\")\n",
    "\n",
    "print(f\"\\nTop 10 most common localizations:\")\n",
    "print(df[\"localization\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac088c5d",
   "metadata": {},
   "source": [
    "The following code is for debugging purposes and can be removed. It simply saves the full localization distribution to a text file for later analysis. This is useful to understand the distribution of localizations in the dataset, and to ensure that the filtering is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/localization_distribution.txt\", \"w\") as f:\n",
    "    f.write(\"Full localization distribution:\\n\")\n",
    "    f.write(df[\"localization\"].value_counts().to_string())\n",
    "    print(\n",
    "        f\"\\nFull localization distribution saved to data/localization_distribution.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923349a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization: convert string columns to categorical to save memory\n",
    "df['entry_name'] = df['entry_name'].astype('category')\n",
    "df['localization'] = df['localization'].astype('category')\n",
    "\n",
    "print(f\"Optimized memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nQuick data exploration:\")\n",
    "print(f\"Sequence length distribution:\")\n",
    "print(df['sequence'].str.len().describe())\n",
    "\n",
    "print(f\"\\nData quality checks:\")\n",
    "print(f\"Entries with very short sequences (<50 AA): {(df['sequence'].str.len() < 50).sum()}\")\n",
    "print(f\"Entries with very long sequences (>2000 AA): {(df['sequence'].str.len() > 2000).sum()}\")\n",
    "\n",
    "print(f\"\\nSample of processed data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ea627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_FASTA, \"w\") as out:\n",
    "    for _, row in df.iterrows():\n",
    "        header = f\">{row['entry_name']}|{row['localization']}\"\n",
    "        seq = row[\"sequence\"]\n",
    "        out.write(f\"{header}\\n\")\n",
    "\n",
    "        for i in range(0, len(seq), 80):\n",
    "            out.write(seq[i : i + 80] + \"\\n\")\n",
    "\n",
    "print(f\"Wrote FASTA to {OUTPUT_FASTA}\")\n",
    "print(f\"Generated {len(df)} sequences in FASTA format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f62b1",
   "metadata": {},
   "source": [
    "We now use [CD-HIT](https://github.com/weizhongli/cdhit/) to cluster the UniProtKB/Swiss-Prot FASTA file at 90% sequence identity, which is a common practice to reduce redundancy in protein datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c0036",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "INPUT_FASTA=\"data/processed/filtered.fasta\"\n",
    "OUTPUT_FASTA=\"data/processed/nonredundant.fasta\"\n",
    "THREADS=4\n",
    "\n",
    "echo \"Running CD-HIT...\"\n",
    "cd-hit -i \"$INPUT_FASTA\" \\\n",
    "       -o \"$OUTPUT_FASTA\" \\\n",
    "       -c 0.90 -n 5 \\\n",
    "       -M 16000 -T $THREADS\n",
    "\n",
    "if command -v cd-hit &> /dev/null; then\n",
    "    echo \"CD-HIT is installed, proceeding with clustering...\"\n",
    "    cd-hit -i \"$INPUT_FASTA\" \\\n",
    "       -o \"$OUTPUT_FASTA\" \\\n",
    "       -c 0.90 -n 5 \\\n",
    "       -M 16000 -T $THREADS\n",
    "    echo \"Clustering completed, nonredundant FASTA at $OUTPUT_FASTA\"\n",
    "else\n",
    "    echo \"CD-HIT is not installed. Please install it to perform clustering.\"\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eslp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
